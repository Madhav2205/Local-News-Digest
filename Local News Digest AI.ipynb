{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45A5HSkVmdo_"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade crewai langchain langchain-community openai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "xMDh5MFBmlrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-\"  # replace with your OpenAI key\n"
      ],
      "metadata": {
        "id": "2P8VPHWqmoEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Agent\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.2)\n",
        "\n",
        "web_scraper = Agent(\n",
        "    role=\"Web Scraper\",\n",
        "    goal=\"Extract latest local news articles\",\n",
        "    backstory=\"You crawl news websites and return their titles and full text.\",\n",
        "    verbose=True,\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "summarizer = Agent(\n",
        "    role=\"Summarizer\",\n",
        "    goal=\"Summarize news articles into concise points\",\n",
        "    backstory=\"You convert long articles into short summaries for a newsletter.\",\n",
        "    verbose=True,\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "classifier = Agent(\n",
        "    role=\"Category Classifier\",\n",
        "    goal=\"Categorize each article\",\n",
        "    backstory=\"You classify news articles into categories like politics, sports, or tech.\",\n",
        "    verbose=True,\n",
        "    llm=llm\n",
        ")\n"
      ],
      "metadata": {
        "id": "D0U5zPMAmqd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Task\n",
        "\n",
        "scrape_task = Task(\n",
        "    description=\"Scrape the latest local news from The Hindu or NDTV and return title + article body.\",\n",
        "    expected_output=\"A list of news articles with their titles and full text.\",\n",
        "    agent=web_scraper\n",
        ")\n",
        "\n",
        "summarize_task = Task(\n",
        "    description=\"Summarize each article retrieved by the Web Scraper.\",\n",
        "    expected_output=\"A list of summaries for each article.\",\n",
        "    agent=summarizer,\n",
        "    depends_on=[scrape_task]\n",
        ")\n",
        "\n",
        "classify_task = Task(\n",
        "    description=\"Categorize each summarized article into appropriate categories.\",\n",
        "    expected_output=\"A list of categories (e.g., Politics, Sports, Tech) corresponding to each summary.\",\n",
        "    agent=classifier,\n",
        "    depends_on=[summarize_task]\n",
        ")\n"
      ],
      "metadata": {
        "id": "jxfEjSHQmtJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Crew\n",
        "\n",
        "crew = Crew(\n",
        "    agents=[web_scraper, summarizer, classifier],\n",
        "    tasks=[scrape_task, summarize_task, classify_task],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "result = crew.kickoff()\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "IV6zUmVsmwUI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}